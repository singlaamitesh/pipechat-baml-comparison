"""
Voice-enabled BAML agent implementation using Pipecat framework.
This agent demonstrates voice fact-checking using structured BAML prompts.
"""
import asyncio
import time
from typing import Dict, Any, Optional, List

from pipecat.frames.frames import LLMMessagesFrame, TextFrame, TTSFrame
from pipecat.processors.base_processor import BaseProcessor
from pipecat.services.ai_services import LLMService
from pipecat.services.openai import OpenAILLMService
from pipecat.services.google import GoogleLLMService
from pipecat.services.elevenlabs import ElevenLabsTTSService
from pipecat.services.deepgram import DeepgramSTTService
from pipecat.transports.local.audio import AudioInputTransport, AudioOutputTransport
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineTask

from src.utils.metrics import MetricsCollector, AgentMetrics
from src.config.settings import settings


# Mock BAML implementation for voice interactions
class VoiceBAMLClient:
    """
    Mock BAML client specifically designed for voice interactions.
    
    In a real implementation, this would integrate with actual BAML
    for structured prompt management and type-safe responses.
    """
    
    async def CheckFactVoice(self, statement: str, conversation_context: List[str] = None):
        """Voice-optimized fact-checking with conversational context."""
        
        class VoiceFactResult:
            def __init__(self, classification: str, explanation: str, 
                        confidence: float, conversational_response: str):
                self.classification = classification
                self.explanation = explanation
                self.confidence = confidence
                self.conversational_response = conversational_response
        
        # BAML would handle this with structured definitions
        # For demo, we'll simulate the structured approach
        if "true" in statement.lower() or any(word in statement.lower() for word in ['earth round', 'chocolate toxic', 'water boil']):
            return VoiceFactResult(
                "True",
                "This statement is factually accurate based on scientific evidence.",
                0.95,
                "That's correct! The statement you mentioned is indeed true."
            )
        elif "false" in statement.lower() or any(word in statement.lower() for word in ['12 fingers', 'ocean reflection', '10% brain']):
            return VoiceFactResult(
                "False", 
                "This statement contradicts established scientific facts.",
                0.90,
                "Actually, that's not accurate. Let me explain why that's incorrect."
            )
        else:
            return VoiceFactResult(
                "Uncertain",
                "Unable to determine factual accuracy with available information.",
                0.50,
                "I'm not entirely sure about that. Could you provide more context?"
            )
    
    async def GenerateConversationalResponse(self, classification: str, explanation: str, user_context: str):
        """Generate voice-optimized conversational responses."""
        
        class ConversationalResult:
            def __init__(self, response: str, follow_up_questions: List[str], tone: str):
                self.response = response
                self.follow_up_questions = follow_up_questions
                self.tone = tone
        
        # BAML would structure this with proper templates
        if classification == "True":
            return ConversationalResult(
                f"You're absolutely right about that! {explanation}",
                ["Would you like to know more details?", "Any other facts you'd like me to check?"],
                "confident"
            )
        elif classification == "False":
            return ConversationalResult(
                f"Actually, that's not quite right. {explanation}",
                ["Would you like to know the correct information?", "Do you have any other questions?"],
                "helpful_corrective"
            )
        else:
            return ConversationalResult(
                f"That's an interesting question. {explanation}",
                ["Could you provide more details?", "Is there a specific aspect you're curious about?"],
                "inquisitive"
            )


class VoiceBAMLFactCheckerProcessor(BaseProcessor):
    """
    Voice-enabled BAML fact-checking processor using structured prompts.
    
    This demonstrates the BAML approach with voice I/O through Pipecat.
    """
    
    def __init__(self, llm_service: LLMService, metrics_collector: Optional[MetricsCollector] = None):
        super().__init__()
        self.llm_service = llm_service
        self.metrics_collector = metrics_collector or MetricsCollector()
        self.baml_client = VoiceBAMLClient()
        self.conversation_history = []
        
    async def process_frame(self, frame, direction):
        """Process incoming frames using BAML structured approach."""
        
        if isinstance(frame, TextFrame):
            # This is transcribed speech from the user
            user_text = frame.text
            
            # Start timing for metrics
            start_time = self.metrics_collector.start_timer()
            
            try:
                # BAML APPROACH: Structured prompt handling
                # Clean separation of concerns, type-safe responses
                fact_result = await self.baml_client.CheckFactVoice(
                    statement=user_text,
                    conversation_context=self.conversation_history[-3:]  # Last 3 exchanges
                )
                
                # Generate conversational response using BAML
                conversational_result = await self.baml_client.GenerateConversationalResponse(
                    classification=fact_result.classification,
                    explanation=fact_result.explanation,
                    user_context=user_text
                )
                
                # BAML provides structured, validated responses
                response_text = conversational_result.response
                
                # Update conversation history
                self.conversation_history.extend([
                    {"role": "user", "content": user_text},
                    {"role": "assistant", "content": response_text}
                ])
                
                # Create TTS frame with the response
                tts_frame = TTSFrame(response_text)
                await self.push_frame(tts_frame, direction)
                
                # Record metrics
                response_time = self.metrics_collector.measure_latency(start_time)
                self._record_interaction_metrics(
                    user_text, response_time, True, 
                    fact_result.classification, fact_result.confidence
                )
                
            except Exception as e:
                # Error handling with BAML structure
                error_response = "I apologize, but I'm having trouble processing that right now. Could you try rephrasing your question?"
                tts_frame = TTSFrame(error_response)
                await self.push_frame(tts_frame, direction)
                
                response_time = self.metrics_collector.measure_latency(start_time)
                self._record_interaction_metrics(user_text, response_time, False, error_msg=str(e))
        
        # Pass through other frames
        await self.push_frame(frame, direction)
    
    def _record_interaction_metrics(self, user_input: str, response_time: float, 
                                  success: bool, classification: str = None, 
                                  confidence: float = None, error_msg: Optional[str] = None):
        """Record metrics for this voice interaction."""
        
        # BAML provides structured data for better metrics
        accuracy = success and (confidence > 0.8 if confidence else True)
        handoff_success = success
        
        metrics = AgentMetrics(
            agent_type="voice_baml",
            statement=user_input,
            latency=response_time,
            accuracy=accuracy,
            handoff_success=handoff_success,
            response_time=response_time,
            tokens_used=None,
            error_message=error_msg
        )
        
        self.metrics_collector.add_metrics(metrics)


class VoiceBAMLFactCheckerAgent:
    """
    Complete voice-enabled BAML fact-checking agent using Pipecat.
    
    This demonstrates how to build a voice agent with the structured 
    BAML prompting approach.
    """
    
    def __init__(self, metrics_collector: Optional[MetricsCollector] = None):
        self.metrics_collector = metrics_collector or MetricsCollector()
        self.pipeline = None
        self.task = None
        
        # Initialize services based on configuration
        self._setup_services()
    
    def _setup_services(self):
        """Setup LLM, TTS, and STT services based on configuration."""
        
        # Setup LLM service
        if settings.LLM_PROVIDER.lower() == "openai" and settings.OPENAI_API_KEY:
            self.llm_service = OpenAILLMService(
                api_key=settings.OPENAI_API_KEY,
                model=settings.DEFAULT_MODEL_OPENAI
            )
        elif settings.LLM_PROVIDER.lower() == "gemini" and settings.GOOGLE_API_KEY:
            self.llm_service = GoogleLLMService(
                api_key=settings.GOOGLE_API_KEY,
                model=settings.DEFAULT_MODEL_GEMINI
            )
        else:
            print("Warning: No valid LLM configuration found, using mock service")
            self.llm_service = None
        
        # Setup TTS service (optional)
        if settings.ELEVENLABS_API_KEY:
            self.tts_service = ElevenLabsTTSService(api_key=settings.ELEVENLABS_API_KEY)
        else:
            print("Info: No ElevenLabs API key, TTS will be limited")
            self.tts_service = None
        
        # Setup STT service (optional)
        if settings.DEEPGRAM_API_KEY:
            self.stt_service = DeepgramSTTService(api_key=settings.DEEPGRAM_API_KEY)
        else:
            print("Info: No Deepgram API key, STT will be limited")
            self.stt_service = None
    
    async def create_voice_pipeline(self):
        """Create the voice processing pipeline."""
        
        if not self.llm_service:
            raise RuntimeError("No LLM service available for voice pipeline")
        
        # Create the BAML fact-checking processor
        fact_checker = VoiceBAMLFactCheckerProcessor(
            llm_service=self.llm_service,
            metrics_collector=self.metrics_collector
        )
        
        # Create audio transports
        audio_input = AudioInputTransport()
        audio_output = AudioOutputTransport()
        
        # Build pipeline components
        pipeline_components = [audio_input]
        
        # Add STT if available
        if self.stt_service:
            pipeline_components.append(self.stt_service)
        
        # Add our BAML fact-checking processor
        pipeline_components.append(fact_checker)
        
        # Add LLM service
        pipeline_components.append(self.llm_service)
        
        # Add TTS if available
        if self.tts_service:
            pipeline_components.append(self.tts_service)
        
        # Add audio output
        pipeline_components.append(audio_output)
        
        # Create pipeline
        self.pipeline = Pipeline(pipeline_components)
        
        return self.pipeline
    
    async def start_voice_session(self, duration_seconds: int = 60):
        """
        Start a voice fact-checking session.
        
        Args:
            duration_seconds: How long to run the session
        """
        if not self.pipeline:
            await self.create_voice_pipeline()
        
        print(f"ðŸŽ¤ Starting voice BAML fact-checking session for {duration_seconds} seconds...")
        print("Say something to fact-check, or ask a factual question!")
        
        # Create and run the pipeline task
        self.task = PipelineTask(self.pipeline)
        
        # Run for specified duration
        try:
            runner = PipelineRunner()
            await runner.run(self.task, duration=duration_seconds)
        except KeyboardInterrupt:
            print("\nâ¹ï¸ Voice session stopped by user")
        except Exception as e:
            print(f"\nâŒ Voice session error: {e}")
        finally:
            if self.task:
                await self.task.stop()
    
    async def run_test_conversation(self, test_statements: List[str]) -> Dict[str, Any]:
        """
        Run a test conversation with predefined statements.
        
        This simulates voice input for testing purposes.
        """
        results = []
        session_start = time.time()
        
        print(f"ðŸ§ª Running voice BAML agent test with {len(test_statements)} statements...")
        
        for i, statement in enumerate(test_statements):
            print(f"\nðŸ“¢ Test statement {i+1}: {statement}")
            
            # Simulate processing the statement
            start_time = self.metrics_collector.start_timer()
            
            try:
                # Process using BAML approach
                baml_client = VoiceBAMLClient()
                
                # Use BAML structured processing
                fact_result = await baml_client.CheckFactVoice(statement)
                conversational_result = await baml_client.GenerateConversationalResponse(
                    fact_result.classification, fact_result.explanation, statement
                )
                
                response_time = self.metrics_collector.measure_latency(start_time)
                
                # Record result with BAML structure
                results.append({
                    "statement": statement,
                    "approach": "voice_baml",
                    "classification": fact_result.classification,
                    "confidence": fact_result.confidence,
                    "conversational_response": conversational_result.response,
                    "tone": conversational_result.tone,
                    "response_time": response_time,
                    "success": True
                })
                
                print(f"âœ… Classification: {fact_result.classification} (confidence: {fact_result.confidence:.2f})")
                print(f"ðŸ“‹ Response: {conversational_result.response}")
                print(f"â±ï¸ Processed in {response_time:.3f}s")
                
            except Exception as e:
                response_time = self.metrics_collector.measure_latency(start_time)
                results.append({
                    "statement": statement,
                    "approach": "voice_baml",
                    "response_time": response_time,
                    "success": False,
                    "error": str(e)
                })
                print(f"âŒ Error: {e}")
        
        session_duration = time.time() - session_start
        
        # Generate summary
        successful_tests = sum(1 for r in results if r["success"])
        avg_response_time = sum(r["response_time"] for r in results) / len(results)
        avg_confidence = sum(r.get("confidence", 0) for r in results if r["success"]) / successful_tests if successful_tests > 0 else 0
        
        summary = {
            "agent_type": "voice_baml",
            "total_tests": len(results),
            "successful_tests": successful_tests,
            "avg_response_time": avg_response_time,
            "avg_confidence": avg_confidence,
            "session_duration": session_duration,
            "results": results,
            "voice_features": [
                "Real-time audio input/output",
                "Speech-to-text processing", 
                "Text-to-speech responses",
                "Conversational interaction flow"
            ],
            "baml_advantages": [
                "Structured prompt definitions",
                "Type-safe response handling",
                "Clean separation of concerns",
                "Confidence scoring built-in",
                "Conversational context management"
            ]
        }
        
        print(f"\n=== Voice BAML Agent Test Summary ===")
        print(f"Tests completed: {successful_tests}/{len(results)}")
        print(f"Average response time: {avg_response_time:.3f}s")
        print(f"Average confidence: {avg_confidence:.2f}")
        print(f"Session duration: {session_duration:.2f}s")
        
        return summary
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get summary of collected metrics."""
        return self.metrics_collector.get_summary()


# Example usage and testing
if __name__ == "__main__":
    async def test_voice_baml_agent():
        """Test the voice BAML agent."""
        agent = VoiceBAMLFactCheckerAgent()
        
        # Test statements
        test_statements = [
            "Is the Earth round?",
            "Tell me if humans have 12 fingers",
            "I heard that water boils at 100 degrees Celsius",
            "Is it true that the sky is blue because of ocean reflection?",
            "Can you verify that chocolate is toxic to dogs?"
        ]
        
        # Run test conversation
        results = await agent.run_test_conversation(test_statements)
        
        # Show results
        print(f"\nðŸ“Š Voice BAML agent completed {results['successful_tests']}/{results['total_tests']} tests")
        print(f"Average response time: {results['avg_response_time']:.3f}s")
        print(f"Average confidence: {results['avg_confidence']:.2f}")
        
        return results
    
    # Run the test
    asyncio.run(test_voice_baml_agent())
